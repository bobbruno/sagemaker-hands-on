{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on a Spark ML Model\n",
    "\n",
    "Following the previous notebook, we'll now deploy the model to a live endpoint. Notice that this notebook does not use the EMR Cluster anymore - in fact, it can't connect to it. The \"Python 3(Data Science)\" kernel is the best choice for executing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use the same code we used on the training notebook to retrieve the cluster and bucket. We'll only need the bucket nane, to retrieve the model tarball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The EMR Cluster Id is j-22XVHS53FXPU1\n",
      "The S3 bucket EMR has access to is studio-emr-v5-s3bucket-t2wfrxqwzj0g\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# This will work only if there's a single created stack in the account, otherwise it will get the first one.\n",
    "cfn = boto3.client('cloudformation')\n",
    "stack_name = cfn.list_stacks(StackStatusFilter=['CREATE_COMPLETE'])['StackSummaries'][0]['StackName']\n",
    "emr_cluster_id = cfn.describe_stack_resource(\n",
    "    StackName=stack_name,\n",
    "    LogicalResourceId='EMRCluster'\n",
    ")['StackResourceDetail']['PhysicalResourceId']\n",
    "emr_bucket = cfn.describe_stack_resource(\n",
    "    StackName=stack_name,\n",
    "    LogicalResourceId='S3Bucket'\n",
    ")['StackResourceDetail']['PhysicalResourceId']\n",
    "emr_cluster_id = \"j-22XVHS53FXPU1\"\n",
    "\n",
    "print(f'The EMR Cluster Id is {emr_cluster_id}\\nThe S3 bucket EMR has access to is {emr_bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below declares the schema for input and output for the endpoint. It is required by MLeap to work, more details can be found on the [Spark ML Service Container readme](https://github.com/aws/sagemaker-sparkml-serving-container#procedure-to-pass-the-schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"name\": \"review\", \"type\": \"string\"}], \"output\": {\"name\": \"prediction\", \"type\": \"double\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "schema = {\n",
    "    \"input\":  [{\"name\": \"review\", \"type\": \"string\"}],\n",
    "    \"output\": {\"name\": \"prediction\", \"type\": \"double\"}\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll set up the model location and sagemaker required information for creating the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# if get_execution_role fails, go to the SageMaker Studio Console\n",
    "# (https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/studio) and copy the execution role from there\n",
    "role = get_execution_role()\n",
    "\n",
    "model_bucket = emr_bucket\n",
    "model_path = \"emr/movie_reviews/mleap\"\n",
    "model_file = \"model.tar.gz\"\n",
    "\n",
    "# S3 location of where you uploaded your trained and serialized SparkML model\n",
    "sparkml_data = f\"s3://{model_bucket}/{model_path}/{model_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model itself. We'll first instantiate a `SparkMLModel` object, passing the tarball, role, session and a name to it. The name has to be unique, therefore we normally use a timestamp in case of rerunning the code. Notice that the SparkML Model receives one additional `env` parameter, where we pass the schema for the endpoint.\n",
    "\n",
    "We also redefined the serializer to JSON, to make it REST-compatible. By default, Spark ML endpoints expect CSV input.\n",
    "\n",
    "*Notice: model deployment takes a few minutes. You can follow it on the left bar, by clicking the \"SageMaker Components and Registries\" icon and select \"Endpoints\", or on the [console](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model_name = \"sparkml-mvreviews-\" + timestamp_prefix\n",
    "sparkml_model = SparkMLModel(\n",
    "    model_data=sparkml_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    name=model_name,\n",
    "    # passing the schema defined above by using an environment\n",
    "    # variable that sagemaker-sparkml-serving understands\n",
    "    env={\"SAGEMAKER_SPARKML_SCHEMA\": schema_json},\n",
    ")\n",
    "\n",
    "\n",
    "endpoint_name = \"sparkml-mvreviews-ep-\" + timestamp_prefix\n",
    "predictor = sparkml_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", endpoint_name=endpoint_name, serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll try one simple string. Go ahead and change the text, just make sure to respect the format (all low-caps, no punctuation, just one space between each word). This is in line with the preprocessing we did before. In a real production environment, we'd create a lambda to execute the same transformations and then call the model, and we'd use the lambda as the exposed endpoint.\n",
    "\n",
    "Legend:\n",
    "- Positive sentiment: 1\n",
    "- Negative sentiment: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"data\": [\"this movie is amazing the best 2 hours of my life\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(float(predictor.predict(payload).decode('utf8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a byte stream, containing the values '1.0' or '0.0'. With some string processing and typecasting, we get a 1 or 0. Let's use the endpoint to predict on the validation data we saved before. First, we load the csv. Since Spark always saves on a subfile and Pandas' `read_csv` can't expand wildcards on s3, we'll have to scan the bucket to determine the exact file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading s3://studio-emr-v5-s3bucket-t2wfrxqwzj0g/emr/movie_reviews/output/predictions.csv/part-00000-8835aa34-078a-4391-87b1-68f58683663b-c000.csv...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(emr_bucket)\n",
    "key = next(obj.key for obj in bucket.objects.all() if 'predictions.csv' in obj.key and obj.key.endswith('.csv'))\n",
    "print(f\"Loading s3://{emr_bucket}/{key}...\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    f\"s3://{emr_bucket}/{key}\"\n",
    ").drop('prediction', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the predictions for the entire dataset. This can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'] = df.review.apply(lambda x: int(float(predictor.predict({\"data\": [x]}).decode('utf8'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we check the accuracy of the predictor (note: it's probably not so great - we're just showing how to deploy a model, not how to optimize it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8603314621758889"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['correct'] = df.prediction == df.label\n",
    "\n",
    "df.correct.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumers of the endpoint can use the boto3 `sagemaker-runtime` client to invoke it. That doesn't require the SageMaker SDK and has very little overhead. It's the recommended way to access SageMaker endpoint from lambda and other clients that don't need the full SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "smrun = boto3.client(\"sagemaker-runtime\")\n",
    "prediction = int(float(smrun.invoke_endpoint(\n",
    "    EndpointName=sparkml_model.endpoint_name, Body=json.dumps(payload).encode('utf8'), ContentType=\"application/json\"\n",
    ")['Body'].read().decode('utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
